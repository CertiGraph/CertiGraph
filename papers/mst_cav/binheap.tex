\lstset{style=myTinyStyle}

Binary heaps are a classic data structure for imperative programs~\cite{clrs,sedgewick}.  Briefly, binary heaps embed a heap-ordered tree in an array and use arithmetic on indices to navigate between a parent and its left and right children.  In addition to providing the standard \texttt{insert} and \texttt{remove-min}/\texttt{remove-max}\footnote{depending on whether it is a min- or max-ordered heap} operations in logarithmic time, binary heaps can by upgraded to support two nontrivial operations.  First, Floyd's \texttt{heapify} function builds a binary heap from an unordered array in linear time, and as a related upgrade, performs a worst-case linearithmic-time sort using only constant additional space.  Second, they can be upgraded to support logarithmic-time \texttt{decrease-} and \texttt{increase-pri}ority operations, which we generalize straightforwardly into \texttt{edit\_pri}.

Binary heaps are a good fit for our graph algorithms because Dijkstra's and Prim's algorithms need to edit priorities, and a constant-space \texttt{heapsort} is appropriate for the sparse edge-list-represented graphs typically targeted by Kruskal's.  The~C language has poor support for polymorphic higher-order functions, and a binary heap that supports \texttt{edit\_pri} is half as fast as a binary heap that does not.  Accordingly, we implement binary heaps in C three times:
\[
\begin{array}{l@{\qquad}c@{\quad}c@{\quad}c@{\quad}l}
\text{name} & \text{heap order} & \texttt{edit\_pri} & \texttt{heapify} & \text{payload} \\
\hline
\text{basic} & \text{min} & \text{no} & \text{yes} & \texttt{void*} \\
\text{advanced} & \text{min} & \text{yes} & \text{no} & \texttt{int} \\
\text{Kruskal} & \text{max} & \text{no} & \text{yes} & \texttt{int, int} \text{ (\emph{i.e.}, unboxed)}
\end{array}
\]
Priorities are simply of type \texttt{int}.  \note[too much detail, too soon?]{The ``advanced'' implementation requires each item to be tagged to a ``key'', which is used to lookup the item whose priority must be changed; these keys are \texttt{unsigned int}s.}  The Kruskal-specific implementation is stripped down to the bare minimum required to implement \texttt{heapsort} (\emph{e.g.} it does not support \texttt{insert}).  We next overview these verifications, divided into three parts: basic heap operations, \texttt{heapify} and \texttt{heapsort} operations, and the \texttt{edit\_pri} operation.

\subsection{The basic heap operations of insertion and min/max-removal}
\label{sec:heapinsertremove}

Because we are juggling three implementations, we take some care to factor our verification to maximize reuse.  First, each C implementation has its own exchange and comparison functions that handle the nitty-gritty of the payload and choose between a min or max heap.  The following lines are from the ``basic'' implementation, in which the ``payload'' (\texttt{data} field) is of type \texttt{void*}:
\begin{lstlisting}
void exch(unsigned int j, unsigned int k, Item arr[]) {
 int priority = arr[j].priority; void* data = arr[j].data;
 arr[j].priority = arr[k].priority; arr[j].data = arr[k].data;
 arr[k].priority = priority; arr[k].data = data; }

int less(unsigned int j, unsigned int k, Item arr[]) {
 return (arr[j].priority <= arr[k].priority); }
\end{lstlisting}
These C functions are specified as refinements of Gallina functions that exchange polymorphic data in lists and compare objects in an abstract preordered set; we verify them in VST after a little irksome engineering.  The payoff is that the key heap operations, which, following Sedgewick~\cite{sedgewick}, we call \texttt{swim} and \texttt{sink}, can use identical~C code (up to alpha renaming) in all three implementations:
\begin{lstlisting}
void swim(unsigned int k, Item arr[]) {
 while (k > ROOT_IDX && less (k, PARENT(k), arr)) {
  exch(k, PARENT(k), arr); k = PARENT(k);         } }

void sink (unsigned int k, Item arr[], unsigned int available) {
 while (LEFT_CHILD(k) < available) {
  unsigned j = LEFT_CHILD(k);
  if (j+1 < available && less(j+1, j, arr)) j++; $\label{code:sinkplusplus}$
  if (less(k, j, arr)) break; exch(k, j, arr); k = j;        } }
\end{lstlisting}
These functions involve a number of complexities, both at the algorithms level and at the semantics-of-C level.  At the C level, there is the potential for a rather subtle bug in the macros \texttt{ROOT\_IDX}, \texttt{PARENT}, etc.  Abstractly, these are simple: the root is in index 0; the children of $x$ at roughly $2x$ and the parent at roughly~$\frac{x}{2}$, with $\pm1$ as necessary.  The danger is thinking that because the variables are \texttt{unsigned int}, all arithmetic will occur in this domain; in fact we must force the associated constants into \texttt{unsigned int} as well:
\[
\begin{array}{@{}l@{~~}|@{~}l@{}}
\begin{minipage}{0.475\textwidth}
\begin{lstlisting}
#define ROOT_IDX  0u
#define PARENT(x) (x-1u)/2u
\end{lstlisting} \end{minipage} &
\begin{minipage}{0.5\textwidth}
%\lstset{firstnumber=3}
\begin{lstlisting}[firstnumber=3]
#define LEFT_CHILD(x)  (2u*x)+1u
#define RIGHT_CHILD(x) 2u*(x+1u)
\end{lstlisting}
\end{minipage}
\end{array}
\]
A second C-semantics issue is the potential for overflow within \texttt{LEFT\_CHILD} and \texttt{RIGHT\_CHILD} (as well as the increments on line~\ref{code:sinkplusplus}), and underflow within the \texttt{PARENT} macro (if \texttt{x} should ever be 0).  To avoid this overflow, the precondition of \texttt{sink} requires that when $\texttt{k}$ is in bounds (\emph{i.e.}, $\texttt{k} < \texttt{available}$), then $2\cdot(\texttt{available}-1) \leq \texttt{max\_unsigned}$.  An edge case occurs when deleting the last element from a heap ($\texttt{k}=\texttt{available}$); we then require $2\cdot\texttt{k}\leq \texttt{max\_unsigned}$.

At the algorithmic level, both the \texttt{swim} and \texttt{sink} functions involve nontrivial loop invariants; \texttt{sink} is complicated by the further need to support Floyd's \texttt{heapify}, during which a large portion of the array is unordered.  Accordingly, we build Gallina models of both functions and show that they restore heap order given a mostly-ordered input heap.  There are two different versions of ``mostly-ordered''. Specifically, \texttt{swim} uses a ``bottom-up'' version:
\begin{lstlisting}
Definition weak_heapOrdered2 (L : list A) (j : nat) : Prop :=
 (forall i b, i <> j -> nth_error L i = Some b ->
   forall a, nth_error L (parent i) = Some a -> a <<= b) /\
 (grandsOk L j root_idx).
\end{lstlisting}
whereas \texttt{sink} uses a ``top-down'' version:
\begin{lstlisting}
Definition weak_heapOrdered_bounded (L:list A) (k:nat) (j:nat) :=
 (forall i a, i >= k -> i <> j -> nth_error L i = Some a ->
  (forall b, nth_error L (left_child i) = Some b -> a <<= b) /\
  (forall c, nth_error L (right_child i) = Some c -> a <<= c)) /\
 (grandsOk L j k).
\end{lstlisting}
In both cases, the parameter \texttt{j} indicates a ``hole'', at which the heap may not be heap-ordered.  The \texttt{grandsOk} predicate bridges this hole by 
specifying an order between the parent and the children of \texttt{j}:
\begin{lstlisting}
Definition grandsOk (L : list A) (j : nat) (k : nat) : Prop :=
  j <> root_idx -> parent j >= k ->
    forall gs bb, parent gs = j -> nth_error L gs = Some bb ->
      forall a, nth_error L (parent j) = Some a -> a <<= bb.
\end{lstlisting}
The parameter \texttt{k} is used to support Floyd's \texttt{heapify}: it bounds the portion of the list in which elements are heap-ordered (with the exception of \texttt{j}).  The proofs that the Gallina \texttt{swim} and \texttt{sink} can restore (bounded) heap-orderedness involve a number of edge cases, but given the above definitions, go through.  The invariants of the~C versions of \texttt{swim} and \texttt{sink} are stated via the associated 
Gallina versions, thereby delegating all associated heap-ordering proofs to the Gallina versions.

The insertion and remove functions we verify are in fact ``non-checking'' versions (\texttt{insert\_nc} and \texttt{remove\_nc}): their preconditions assume there is room in the heap to add or an item in the heap to remove.  In the context of Dijkstra and Prim, these preconditions can be proven to hold.  The associated verifications involve a little separation logic hackery (specifically, to \textsc{Frame} away the ``junk'' part of the heap-array from the ``live'' part), but are straightforward using VST.  We avoid the overflow issue in \texttt{sink} by bounding the maximum capacity of the heap: $2\cdot(\texttt{capacity} - 1) \leq \texttt{max\_unsigned}$.  We require users to prove this bound on heap creation, and thereafter handle it under the hood.

\subsection{Bottom-up heapify and heapsort}

Floyd's bottom-up procedure for constructing a binary heap in linear type, and using a binary heap to sort, are classics of the literature~\cite{clrs,sedgewick}.  Happily, while the asymptotic bound on heap construction is nontrivial, the implementations of both are basically repeated calls to \texttt{sink} (and exchanges to remove the root):
\begin{lstlisting}
void build_heap(Item arr[], unsigned int size) {
 unsigned int start = PARENT(size); $\label{code:parentunderflow}$
 while(1) { sink(start, arr, size);
            if (start == 0) break; start--;  } }

void heapsort_rev(Item* arr, unsigned int size) {
 build_heap(arr,size); $\label{code:buildheapsort}$
 while (size > 1) { size--;
  exch(ROOT_IDX, size, arr); sink(ROOT_IDX, arr, size); } }
\end{lstlisting}
Given that in \S\ref{sec:heapinsertremove} we already generalized the specification for \texttt{sink} to handle a portion of the array being unordered, the verification of these functions is straightforward.  There is, however, the possibility of a subtle underflow on line~\ref{code:parentunderflow}, in the case when building an empty heap (\emph{i.e.}, $\texttt{size}=0$).  In turn, this means that \texttt{heapsort\_rev} as given above cannot sort empty lists; in our ``basic'' implementation we strengthen the precondition accordingly, whereas in our ``Kruskal'' implementation we add a line before~\ref{code:buildheapsort} that \texttt{return}s when $\texttt{size}=0$.  We use a max-heap for Kruskal because heapsort yields a \emph{reverse} sorted list.

\subsection{Modifying an element's priority}

In contrast to \texttt{heapify} and \texttt{heapsort}, standard algorithms textbooks are vague on the implementation of \texttt{edit\_pri}ority~\cite{clrs,sedgewick}.  The idea is that, during \texttt{insert}, each item is associated not only with its usual \texttt{int} priority
but also a unique \texttt{unsigned int} ``key''. 
This key is generated during \texttt{insert} and returned to the user.
Internally, the binary heap maintains a secondary array \texttt{key\_table} that maps each key to the current heap-array location of the associated item.  
The user calls \texttt{edit\_pri} by supplying the \emph{key} of the item
they wish to modify, and the binary heap uses the key to 
locate the item. To keep everything linked together properly, the \texttt{key\_table} is modified during the \texttt{exch}ange function.

One detail entirely missing from standard textbooks is how to generate the keys on insert.  The initial idea is to have a global counter starting at 0, which is then increased on each insert.  Unfortunately, this is unsound: during (very) long runs involving both \texttt{insert} and \texttt{remove-min}, this key counter will overflow.  Although overflow is defined in C for \texttt{unsigned int}, this overflow is fatal algorithmically: multiple items could be assigned the same key.

A better method is to store a key field within each heap-item in the main array.  These keys are initialized to $0..(\texttt{capacity}-1)$, and thereafter are never modified other than when two cells are swapped during \texttt{exch}.  An invariant can then be maintained: the keys from the ``live'' and ``junk'' parts of the main heap array always form an exact permutation of $0..(\texttt{capacity}-1)$.  On insertion, we ``recycle'' the key of the first ``junk'' item.

%The VST proofs of the C code are only slightly more challenging since the key complexities are encapsulated into the \texttt{exch} function.
