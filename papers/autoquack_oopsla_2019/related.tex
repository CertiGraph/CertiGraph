\paragraph{Comparison with~\citet{hobor:ramification}.}
Our work builds on the theory of ramification by Hobor and Villard,
who verified graph algorithms on pen-and-paper using their \infrulestyle{Ramify} rule:
\begin{equation*}
%\label{eq:ramify}
\inferrule[Ramify]
{\{ L_1 \} ~ c ~ \{ L_2 \} \\
G_1 |- L_1 * (L_2 --* G_2)}
{\{ G_1 \} ~ c ~ \{ G_2 \}} \qquad \mathit{freevars}(L_2 --* G_2) \cap \MV(c) = \emptyset
\end{equation*}
Our \textsc{Localize} rule upgrades \textsc{Ramify} to better handle modified program
variables (note the side condition and recall the discussion in \S\ref{sec:localizations})
and existential quantifiers in postconditions.  Hobor and Villard avoided these challenges
by proposing a unwieldy variant of \infrulestyle{Ramify} called \infrulestyle{RamifyAssign}, which
could reason about the special case of a single assignment $\li{x=}f(\ldots)$, assuming
the verifier can make the local program translation to $\li{x'=}f(\ldots)\li{; x=x'}$,
where \li{x'} is fresh.  This is nontrivial in large existing formal
developments, such as VST, that do not have any way to prove programs equivalent.
Hobor and Villard could not verify unmodified program code, modify program variables
inside nested localization blocks, or handle multiple assignments in a single block as
in lines~\ref{code:markbeforetripleramify}--\ref{code:markaftertripleramify} of
Figure~\ref{fig:markgraph}.  They avoided existentials in localized
postconditions by defining all mathematical operations (\emph{e.g.} $\m{mark}$) as
functions rather than as relations; this is fine for pen-and-paper, but painful in
a mechanized setting wherein functions must be proven to terminate.

Hobor and Villard treated mathematical graphs as triples $(V,E,L)$ of
vertices, edges, and a vertex labeling function, where vertices had no more than two
neighbors. Our mathematical graph framework~(\S\ref{sec:mathgraph}) is more
modular and versatile, and ships with hundreds of reusable definitions and theorems. Further, our library has been tuned to work smoothly in a mechanized context.


Hobor and Villard erroneously defined spatial graphs
recursively. Unfortunately, other members of the research
community (\emph{e.g.}~\citet{raadvg15}) followed their lead.  We expose this
error~(\S\ref{sec:fixpointfail}) and provide a sound and rather general definition for
\p{graph} that recovers fold/unfold reasoning~(\S\ref{sec:goodgraph}).  We develop a
much more general and more modular set of related lemmas and connected our spatial
reasoning to the verification framework of CompCert/VST~(\S\ref{sec:vst}).
Our development is entirely
machine-checked~(\S\ref{sec:development}) whereas they used only pen and paper.

\paragraph{Other Pen-and-Paper Verification of Graph Algorithms and/or $**$.}

\citet{hongseok:phd} verified the Schorr-Waite algorithm, and this 
is widely considered a landmark in the early separation logic literature. 
\citet{bornat:aliasing04}~gave an early attempt to reason about graph algorithms 
in separation logic in a more general way. 
\citet{neelthesis}~provided the first separation logic proof of union-find.

\citet{rey-slnotes}~was the first to document the overlapping 
conjunction $**$, albeit without any strategy to reason about it using Hoare rules. 
\citet{gardnerms12}~were the first to reason about a program using $**$ in 
Javascript. 
\citet{raadvg15}~used $**$ within their CoLoSL program logic to reason about 
a concurrent spanning algorithm using a kind of ``concurrent localization''.

%\paragraph{Alternative fixpoint constructions.}
%Appel and McAllester defined an alternative ``contractive'' fixpoint that is sometimes used to define recursive predicates in separation
%logic~\cite{appel:fixpoint}.  In Appendix~\ref{apx:appelfixpiont} we explain why Appel and McAllester's  is also unsuitable to define graphs.

\paragraph{Machine-Checked Verification of Graph Algorithms.}
A decade after Yang verified Schorr-Waite on paper, \citet{leino10} automated 
its verification in Dafny. 
\citet{ilya-graphs}~verified a concurrent spanning tree algorithm, and 
moreover developed mechanized Coq proofs. Their algorithm was written in FCSL, 
a monadic DSL that combines effectful operations with pure Coq expressions; 
FSCL cannot be executed. 
\citet{chen18}~compared how three provers (Coq, Isabelle, and Why3) can 
verify Tarjan’s strongly-connected component algorithm written in the native 
language of each of the tools. Because these are written in the native languages 
of a proof assistant, they avoid “real-world” language concerns such as 
memory models and overflow.

\citet{lamneu15}~extended the Isabelle Refinement Framework to verify a range of 
DFS algorithms via stepwise refinement.
Their framework allows the reuse of previously-proved DFS 
invariants by establishing an inductive
``most specific invariant'' and deriving other inductive invariants from it.
\citet{lamsef19}~extended this further and presented verifications of 
the correctness and time complexity of the Edmonds-Karp and push-relabel
algorithms. Lammich et al. produced very readable proofs of classic 
textbook algorithms by using the Isar language atop of their Isabelle proofs.
They used Isabelle's code generator to export efficient executable code, 
but with the caveat that the code comes with a guarantee of only 
partial correctness semantics.

\citet{char11}~used his CFML tool to Coq-verify an OCaml implementation of 
Dijkstra. 
\citet{gueneauetal19}~extended CFML and verified the correctness 
and time complexity of a modified version of the BFGT cycle-detection algorithm.
The graph algorithms verified in CFML tend to be ``graph theory'' in flavour, 
whereas the algorithms we have verified tend to have more of a
``systems'' flavor. This difference is partially explained by the fact that 
code written in ML can take advantage of its high-level design, whereas
code written in C is often interested in handling grungy systems tasks. For 
example, references in ML cannot be null and do not support pointer arithmetic; 
of course both are possible---and lead to nontrivial complications---in 
C. Accordingly, the CFML proofs benefit from ML’s cleaner computational model. 
Our verifications are in C so we must contend with C’s memory model, pointer 
arithmetic, significant scope for undefined behavior, and so forth.

\citet{charpott19}~used CFML to verify the correctness and 
time complexity of union-find. Their work is an interesting counterpoint to 
ours because, while it maintains an abstraction between the client and the 
internal mathematical/spatial facts that the client need not know, it does 
not maintain a separation between the mathematical and spatial 
facts themselves, as we do in~\S\ref{sec:mathgraph} and ~\S\ref{sec:spacegraph}.
This separation is worthwhile: our modular method let us verify an alternate 
version of union-find that uses an array of vertices rather than individually 
heap-allocated nodes. This 
secondary verification then used \emph{exactly the same} mathematical proof of 
functional correctness despite the radically different layout of spatial 
memory.
Our work does not verify the time complexity of union-find. When 
we attempted to prove the necessary amortisation bounds we ran into an 
overflow issue: it was impossible to prove that the rank would not exceed 
\li{max\_int} because the CompCert memory model does not place a bound on the 
total number of allocations. Informally, this overflow is impossible in 
practice because no computer has $2^{2^{\tiny 64}}$ bytes of memory, which would 
be required for this overflow to occur, but Coq remains unconvinced. 
Chargu{\'{e}}raud and Pottier acknowledged and sidestepped this issue by 
representing rank using the Coq type \li{Z}, which was not an option for us 
given the end-to-end nature of the VST+CompCert toolchain.

\paragraph{Verification Tools in Coq.}
Our work interacts with the Floyd verification module within the Verified 
Software Toolchain (VST)~\cite{appel:programlogics}. The Floyd module uses 
tactics to enable the separation-logic verification of CompCert C programs. 
VST connects to the CompCert certified C compiler~\cite{leroy:compcert}, and 
thus has no gaps or admits between the verified source code and the eventual
assembly code~\cite{appelvst}.

Charge! likewise uses Coq tactics to work with a shallow embedding of higher 
order separation logic, but focuses on OO programs written in 
Java/C\#~~\cite{bengtson:charge}. Iris Proof Mode provides a similar framework 
for higher-order concurrent reasoning in Coq~\cite{krebbers:iris}.

CFML enables the verification of OCaml programs by reasoning about their
``characteristic formulae'' in separation logic using Coq~\cite{char10, char11}. 
CFML has been used to verify a range of functional and imperative programs,
including some graph-related algorithms as discussed 
above. \citet{charpott19}~extended CFML to reason about time 
credits. The work of \citet{gueneau17}~indicates that CFML is exploring a connection 
with the certified CakeML compiler~\cite{cakeml}.

While the tools above require substantial human guidance, 
Bedrock~\cite{chlipala:bedrock} is a more automated approach to the verification of 
low level programs using separation logic in Coq. 
Bedrock leverages the fact that phrasing function 
specifications in a \emph{computational} style 
(in this case, inspired by functional programming) 
leads to separation logic proof obligations that are quite automatable.
It simplifies these obligations into pure mathematics using a 
custom workhorse tactic, and then discharges those 
obligations using standard Coq automation.

\paragraph{Other Verification Tools.} 

Many more-automated verification tools also use separation logic in a forward
reasoning style. Smallfoot~\cite{berdine:smallfoot}, jStar~~\cite{distefanop08}, 
HIP/SLEEK~\cite{chin:hipsleek}, and Verifast~\cite{jacobs:verifast} are landmarks
at various points on the expressibility-automatability spectrum. 
KeY~\cite{beckert:2007} and Dafny~\cite{leino10} are verifiers that are not 
based on separation logic. KeY uses an interactive verifier while Dafny pursues
 automation with Z3~\cite{moura2008}.

\paragraph{Mechanized Mathematical Graph Theory.}
There is a long history, going back at least 28 years, of mechanized 
reasoning about mathematical graphs~\cite{wong1991}. 
The most famous mechanically verified “graph theorem” is the Four Color 
Theorem~\cite{gonthier2005computer}; however the development actually uses
hypermaps instead of graphs. In general most “mathematical graph” frameworks in 
the literature~\cite{wong1991, chou1994, yamamoto1995formalization, rwpgt1998, yamamoto1998formalization, tamai2000formal, duprat2001coq, ridge2005graphs, nipkow2016, dijkstra_shortest_path-afp} were not used to verify real 
code, for which they seem unsuitable. Verifying real code requires delicate concepts such as removing a subgraph, null nodes, and parallel edges, and one of our contributions is that our framework is general enough to support such verification. 
\citet{noschinski2015}~built a graph library in Isabelle/HOL whose formalization 
is the closest to ours, 
\emph{e.g.} supporting graphs with labeled and parallel arcs. 
Beyond being in Coq, our setup supports at least three features beyond 
Noschinski’s: reasoning about incomplete graphs (as discussed 
in~\S\ref{sec:mathinfra} using figure~\ref{fig:pregraph}), labeling the graph 
as a whole (used, for example, in the garbage collector to store 
metainformation about the number and location of the generations), and our 
modular typeclass-supported “graphs with properties” setup in General Graph 
(as described in~\S\ref{subsec:graphplugins}). 
\citet{dubois2015graphes} and \citet{noschinski2015formalizing}~used proof assistants to 
design verifiable checkers for solutions to graph problems. 
\citet{bauer20025} and \citet{yamamoto1995formalization}~used an inductive encoding of graphs to formalize planar graph theory.


\paragraph{Verification of Garbage Collection Algorithms.}
Schism \cite{gcexample4,gcexample4a} is a certified concurrent
collector built in a Java VM that services multi-core architectures with weak memory consistency.
\citet{gcexample5, gcexample3} introduced GCminor, which is
a certified translation step added to CompCert's translation from Clight to assembly.
GCminor makes explicit the specific invariants that the garbage collector
relies upon, thus minimising errors due to the violation of invariants
between the garbage collector and the mutator.
\citet{gcexample2} annotated x86 code
for two GCs by hand, and then used Boogie and the Z3 automated theorem prover
to verify their correctness automatically.

The closest piece of work to our certified GC is probably the excellent certified GC
for the Cake ML project~\cite{cakemlgc}, since both integrate a certified GC into 
a certified compiler for a functional language.  Their GC is written closer to assembly 
than C, which is both a positive---in that they avoid undefined behaviors---and a negative, 
in that their GC is harder to understand and upgrade and cannot take advantage of the
mature CompCert compiler.  Their GC lacks some of our optimisations (\emph{e.g.} they have 
only three generations), but on the other hand handles mutation in the GC heap.  The largest 
difference, however, is that we present an integrated graph framework suitable for reasoning 
about many graph algorithms, of which our GC is merely the flagship.  In contrast, they focus 
much more narrowly on the problem of certified GCs.
